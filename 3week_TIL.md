# 통계학 3주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_3rd_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

2주차는 `2부-데이터 분석 준비하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.


## Statistics_3rd_TIL

### 2부. 데이터 분석 준비하기
### 08. 분석 프로젝트 준비 및 기획
### 09. 분석 환경 세팅하기



## Study Schedule

|주차 | 공부 범위     | 완료 여부 |
|----|----------------|----------|
|1주차| 1부 p.2~56     | ✅      |
|2주차| 1부 p.57~79    | ✅      | 
|3주차| 2부 p.82~120   | ✅      | 
|4주차| 2부 p.121~202  | 🍽️      | 
|5주차| 2부 p.203~254  | 🍽️      | 
|6주차| 3부 p.300~356  | 🍽️      | 
|7주차| 3부 p.357~615  | 🍽️      |  

<!-- 여기까진 그대로 둬 주세요-->

# 08. 분석 프로젝트 준비 및 기획

```
✅ 학습 목표 :
* 데이터 분석 프로세스를 설명할 수 있다.
* 비즈니스 문제를 정의할 때 주의할 점을 설명할 수 있다.
* 외부 데이터를 수집하는 방법에 대해 인식한다.
```
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->
8.1 데이터 분석의 전체 프로세스

8.1.1 데이터 분석의 3단계
|단계 | 내용     |
|----|----------------|
|설계 단계  |괴제 정의 및 범위 설정     |
|           |인력 구성 및 PM 확보    |
|           |실무자·데이터분석가 간 협의 체계 수립   |
|분석 및 모델링 단계    |데이터 분석을 위한 데이터 MART 구축     |
|                     |데이터 준비·가공·분석 및 모델 도출    |
|                     |모델 검증 및 실무·경영진 협의   |
|구축 및 활용 단계   |모델 적용 및 시스템 구축     |
|                 |성과 평가 및 추가·보완 프로젝트 검토    |

설계 단계: 분석가는 비즈니스 도메인과 데이터에 익숙하지 않은 경우가 대부분이므로 실무자와의 피드백 체계 확립이 중요하다.  
분석 및 모델링 단계: 모델의 비즈니스 적합성을 심도 있게 분석하고, 성능을 평가하는 것이 중요하다.  
-방법론의 종류: KDD, CRISP-DM, SEMMA 등  
구축 및 활용 단계: 실무에 적용하기 위한 IT시스템 구축이 필요한 경우도 있으므로 예상 개선 효과 측정 시 시스템 적용을 위한 비용도 고려한다.

8.1.2 CRISP-DM 방법론

1단계 - 비즈니스 이해  
-현재 상황 평가  
-데이터 마이닝 목표 결정  
-프로젝트 계획 수립

2단계 - 데이터 이해  
-데이터 설명  
-데이터 탐색  
-데이터 품질 확인

3단계 - 데이터 준비  
-데이터 선택  
-데이터 정제  
-필수 데이터 구성  
-데이터 통합

4단계 - 모델링  
-모델링 기법 선정  
-테스트 디자인 생성  
-모델 생성  
-모델 평가

5단계 - 평가  
-결과 평가  
-프로세스 검토  
-다음 단계 결정

6단계 - 배포  
-배포 계획  
-모니터링 및 유지 관리 계획  
-최종 보고서 작성  
-프로젝트 검토

8.1.3 SAS SEMMA 방법론 

Sampling 단계  
-전체 데이터에서 분석용 데이터 추출  
-의미 있는 정보를 추출하기 위한 데이터 분할 및 병합  
-표본추출을 통해 대표성을 가진 분석용 데이터 생성  
-분석 모델 생성을 위한 학습, 검증, 테스트 데이터셋 분할

Exploration 단계  
-통계치 확인, 그래프 생성 등을 통해 데이터 탐색  
-상관분석, 클러스터링 등을 통해 변수 간의 관계 파악  
-분석 모델에 적합한 변수 선정  
-데이터 현황을 파악하여 비즈니스 아이디어 도출 및 분석 방향 수정

Modifiation 단계  
-결측값 처리 및 최종 분석 변수 선정  
-로그변환, 구간화 등 데이터 가공

Modeling 단계  
-다양한 데이터마이닝 기법 적용에 대한 적합성 검토  
-비즈니스 목적에 맞는 분석 모델을 선정하여 분석 알고리즘 적용  
-지도학습, 비지도학습, 강화학습 등 데이터 형태에 따라 알맞은 모델 선정  
-분석 환경 인프라 성능과 모델 정확도를 고려한 모델 세부 옵션 설정

Assessment 단계  
-구축한 모델들의 예측력 등 성능을 비교, 분석, 평가  
-비즈니스 상황에 맞는 적정 임계치 설정  
-분석 모델 결과를 비즈니스 인사이트에 적용  
-상황에 따라 추가적인 데이터 분석 수행


-> 8.2 ~ : 각 단계에 대한 이해  
8.2 비즈니스 문제 정의와 분석 목적 도출

비즈니스 문제는 현상에 대한 설명으로 끝나서는 안 되고, 본질적인 문제점이 함께 전달되어야 하는 것이다.

MECE Mutually Exclusive Collectively Exhausitive  
: 세부 정의들이 서로 겹치지 않고 전체를 합쳤을 때 빠진 것 없이 완전히 전체를 이루는 것.  
비즈니스 문제를 올바르게 정의하기 위한 논리적 접근법으로 가장 널리 쓰이는 방식.  
-일반적으로 로직 트리를 활용하여 세부 항목을 정리한다.

분석 과제들을 도출한 후, 상황에 따른 우선순위를 측정해 프로젝트를 수행한다.  
우선순위 결정방식인 페이오프 매트릭스(Pay off Matrix) 상의 Grand Slam을 우선시하며  
Extra Innings의 일부를 중장기 과제로 선정하고 Stolen Base와 Strike Out은 제외한다.

|-|실행가능성 높음|실행가능성 낮음|
|----|----------------|------------|
|사업 성과 높음|Grand Slam|Extra Innings|
|사업 성과 낮음|Stolen Base|Strike Out|

8.3 분석 목적의 전환  
분석 목적 설정이 데이터 탐색 이전에 이루어진 경우,   
이후 데이터 탐색을 통해 숨겨진 정보나 인사이트를 얻게 될 수 있으며,  
이로 인해 분석 프로젝트의 방향이 바뀔 수 있다.

8.4 도메인 지식

8.5 외부 데이터 수집과 크롤링  
외부데이터 수집 방법: 데이터 구매 / 오픈 데이터 수집 / 크롤링(=스크래핑)

크롤링 시 허용 범위에 대한 파일: robots.txt  
-User-agent: 대상 크롤러 (모든 검색 봇, 구글 봇 등)  
-Allow: 허용하는 경로  
-Disallow: 허용하지 않는 경로


# 09. 분석 환경 세팅하기

```
✅ 학습 목표 :
* 데이터 분석의 전체적인 프로세스를 설명할 수 있다.
* 테이블 조인의 개념과 종류를 이해하고, 각 조인 방식의 차이를 구분하여 설명할 수 있다.
* ERD의 개념과 역할을 이해하고, 기본 구성 요소와 관계 유형을 설명할 수 있다.
```

<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->
9.1 데이터 분석 언어  
SAS, R, Python, SQL

9.2 데이터 처리 프로세스

데이터베이스 서버 환경 하에서의 데이터 흐름: OLTP -> DW(ODS) -> DM -> OLAP  
-OLTP On-Line Transaction Processing: 트랜잭션 단위의 데이터를 실시간 수집, 분류, 저장하는 시스템. 데이터가 생성되고 저장되는 첫 단계.  
-DW Data Warehouse: 수집된 데이터를 사용자 관점에서 주제별로 통합하여 추출하기 용이하게 저장해 놓은 통합 데이터베이스. 전체 히스토리 데이터 보관.  
-ODS Operational Data Store: 데이터를 DW에 저장하기 전 임시로 데이터를 보관하는 중간 단계의 저장소. 최신 데이터를 반영하는 목적.  
-DM Data Mart: 사용자의 목적에 맞도록 가공된 일부의 데이터가 저장되는 곳. 사용자 집단의 필요에 맞도록 가공된 개별 데이터 저장소. 접근성과 데이터 분석의 효율성을 높이고 DW 시스템 부하를 감소시키는 기능을 한다.

기본적 데이터 처리 프로세스  
ETL (Extract, Transform, Load): 저장된 데이터를 사용자가 요구하는 포맷으로 변형하여 이동시키는 작업 과정.  
-Extract: 원천 소스 데이터베이스로부터 필요한 데이터를 읽어 들이는 과정.  
-Transform: 미변환 상태의 raw 데이터를 정리, 필터링, 정형화하고 요약하여 분석에 적합한 상태로 바꾸어 놓는 과정.  
-Load: 변환된 데이터를 새로운 테이블에 적재하는 과정.


9.3 분산데이터 처리  
분산 데이터 처리: 한 컴퓨터가 처리해야 할 일을 여러 컴퓨터가 나눠서 한 다음 그 결과를 합치는 과정  
-scale-up 방식: 하나의 컴퓨터의 용량을 늘리고 더 빠른 프로세서를 탑재하는 방식.  
-scale-out 방식: 여러 대의 컴퓨터를 병렬적으로 연결하는 방식.

분산 시스템 구조  
: 하나의 컴퓨터가 하나의 노드일 때, 노드들이 합친 것이 랙, 랙들이 모인 것이 클러스터가 된다.  
클라이언트가 하나의 잡을 실행시키면 해당 잡은 여러 개의 태스크로 분리되는데, 각 태스크는 맵과 리듀스를 통해 분산 처리된다.


9.3.1 HDFS  
HDFS Hadoop Distributed File System  
: 대표적 분산처리 기술. Slave node, Master node, Client machines의 세 모듈로 나눠진다.  
-Slave node: 데이터를 저장하고 계산.  
-Master node: 대량의 데이터를 HDFS에 저장하고 맵리듀스 방식을 통해 데이터를 병렬처리.  
-Client machines: 맵리듀스 작업을 통해 산출된 결과를 사용자에게 출력.

맵리듀스 Mapreduce  
: 맵과 리듀스의 두 단계로 구성되어 데이터를 효과적으로 처리하는 방식.   
구글에서 발표한 논문을 바탕으로 아파치 오픈소스 프로젝트에서 하둡 맵리듀스가 개발되었다.  
-맵: 흩어져 있는 데이터를 관련된 데이터끼리 뭈어 임시의 집합을 만드는 과정.  
-리듀스: 필터링과 정렬을 거쳐 데이터를 뽑아내는 과정.  
-Key-Value 쌍으로 데이터를 처리하는 특징

외에도 분산 코디네이터 Zookeeper, 분산 리소스 관리 YARN, 데이터처리 pig, Mahout, Hive 등의 오픈소스 시스템이 존재한다.

하둡2.0의 YARN은 JobTracker를 리소스 매니저, 애플리케이션 마스터, 타임라인 서버 등으로 분리하여 기능을 고도화 했다.  
-JobTracker: 기본적인 리소스 관리 시스템. 전체 클러스터의 리소스 관리, 수행 중인 잡들의 진행상황, 에러 관리, 완료된 잡들의 로그 저장 및 확인 등을 수행한다.  
-애플리케이션 마스터: 각 클로스터마다 존재하여 여러 잡들이 성공적으로 수행될 수 있도록 리소스 관리와 스케줄링, 모니터링 기능 등을 제공한다.  
-리소스 매니저: 잡에 필요한 자원을 할당한다.  
-타임라인 서버: 잡에 대한 로그 이력을 관리한다.  
-노드 매니저: 모든 노드에서 실행되어 각각의 할당된 태스크를 실행하고 진행 상황을 관리한다.


9.3.2 아파치 스파크

HDFS는 데이터 전송, 분산 파일 시스템, 분산 데이터 처리, 운영 관리 레이어로 구분된다.  
스파크는 분산 데이터 처리를 담당하는 하나의 시스템이다.  
기존 하둡의 맵리듀스 방식보다 배치 처리 작업은 10배, 인메모리 분석은 100배 빠르다.  
기존의 맵리듀스 방식은 데이터 처리 프로세스 관점의 데이터가 디스크에 남아있는 반면 스파크는 메모리에 저장하여 재사용이 가능하기 때문에,  
머신러닝과 같은 반복형이나 대화형 태스크에서 뛰어난 성능을 보인다.

-인메모리 기반의 빠른 데이터 처리가 가능하다.  
-다양한 언어를 지원함으로써 사용이 편리하다.  
-데이터 마이닝과 같은 온라인 분석처리 작업에 특화됐으나 대량의 온라인 트랜잭션 처리와 같은 대량의 원자성 트랜잭션 처리에는 적합하지 않다.

데이터 분석가는 스파크 환경에서 웹 기반 노트북이자 시각화 툴인 제플린을 주로 사용한다.


9.4.1 테이블 조인
-레프트 조인과 라이트 조인: 하나의 테이블을 기준으로 다른 테이블에서 겹치는 부분을 결합한다.  
일치하는 키 값이 없는 행은 조인하는 테이블의 값이 결측값으로 나타난다.  
결합하는 테이블의 키 값에 해당하는 관측치가 여러 개일 경우 그만큼 행이 추가된다.

-이너 조인과 풀 조인: 이너조인은 겹치는 부분의 행만, 풀 조인은 모든 행을 가져온다.

-크로스 조인: 한 테이블의 행마다 다른 테이블의 행을 각각 결합하여 행을 생성한다.

9.4.2 데이터 단어사전

데이터 단어사전: 각 칼럼과 테이블의 이름을 정할 때 체계를 약속한 일종의 사전.  
메타데이터 관리 시스템: 데이터가 어디에 어떻게 저장되어 있는지, 데이터를 어떻게 사용할 것인지 이해할 수 있도록 데이터에 대한 정보를 관리하는 시스템.  
테이블 정의서: 메타데이터 관리 시스템을 간소화한 버전. 각 DW, DM 등에 적재된 테이블과 칼럼의 한글과 영문명, 데이터 속성, 간단한 설명 등이 정리된 표.

ERD Entity Relationship Diagram: 각 테이블의 구성 정보와 테이블 간 관계에 대한 도식  
ERD의 핵심은 테이블 간 연결을 해주는 키 칼럼과 연결 관계를 의미하는 식별자이다.  
-물리 ERD: DB를 효율적이고 결점 없이 구현하는 것을 목표로 하는 것  
-논리 ERD: 데이터 사용자 입장에서 테이블 간 매핑에 오류가 없으며 데이터 정규화가 이루어진 개념

![image](https://github.com/user-attachments/assets/4dd86be6-ce97-4bcc-a9c2-f2ef441b0d6b)

<br>
<br>

# 확인 문제

## 문제 1.

> **🧚 아래의 테이블을 조인한 결과를 출력하였습니다. 어떤 조인 방식을 사용했는지 맞춰보세요.**

> 사용한 테이블은 다음과 같습니다.

![TABLE1](https://github.com/ejejbb/Template/raw/main/File/2.6.PNG)|![TABLE2](https://github.com/ejejbb/Template/raw/main/File/2.7.PNG)
---|---|

> 보기: INNER, LEFT, RIGHT 조인

<!-- 테이블 조인의 종류를 이해하였는지 확인하기 위한 문제입니다. 각 테이블이 어떤 조인 방식을 이용하였을지 고민해보고 각 테이블 아래에 답을 작성해주세요.-->

### 1-1. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-1.PNG)
```
LEFT
```

### 1-2. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-3.PNG)
```
INNER
```

### 1-3. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-2.PNG)
```
RIGHT
```

### 🎉 수고하셨습니다.
