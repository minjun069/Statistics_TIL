
## 11. 데이터 전처리와 파생변수 생성

### 11.1 결측값 처리  

결측값이 발생하는 경우의 분류  

-완전 무작위 결측 MCAR Missing Completely at Random  
: 순수하게 결측값이 무작위로 발생한 경우. 결측값을 제거해도 편향이 거의 발생하지 않는다.  
-무작위 결측 MAR Missing at Random  
: 다른 변수의 특성에 의해 해당 변수의 결측치가 체계적으로 발생한 경우.  
-비무작위 결측 NMAR Missing at Not Random  
: 결측값들이 해당 변수 자체의 특성을 갖고 있는 경우.

결측값 처리 방법

단순 대치법  
-표본제거 방법: 전체 데이터에서 결측값 비율이 10% 미만인 경우 이 방법을 사용한다.  
-평균 대치법  
-보간법  
-회귀대치법, 확률적 회귀대치법

다중 대치법: 단순 대치를 여러 번 수행하여 이들의 평균으로 결측값을 대치.  
대치단계, 분석 단계, 결합 단계의 프로세스를 거친다.  
일반적으로 몬테카를로 방법 또는 연쇄방정식을 통한 다중 대치 MICE 를 사용하여 대치값을 임의로 생성한다.


### 11.2 이상치 처리

이상치 처리 방법  
-이상치 제거  
-관측값 변경  
-가중치 조정

이상치 식별 방법  
-MAD Median Absolute Deviation  
-이상치 변수화: 이상치에 대한 설명력을 추가하는 방법  
-논리적 식별 등


### 11.3 변수 구간화 Binning  
데이터 분석의 성능을 향상시키거나 해석의 편리성을 위해 이산형 변수를 범주형 변수로 변환하는 방법.  
비즈니스 상황에 맞도록 변환시킴으로써 데이터의 해석이나 예측, 분류 모델을 의도에 맞도록 유도.

구간화 방법  
-특정 간격, 특정 의미 기준  
-각 범주 별 수가 비슷해지도록 구간화  
-이산 값을 평활화하여 단순 이산 값으로 변환  
-클러스터링, 의사결정나무 등 머신러닝 기법

WOE, IV 값 등을 통해 변수값이 효과적으로 구간화됐는지 측정 가능


### 11.4 데이터 표준화와 정규화 스케일링  
독립 변수들이 서로 단위가 다르거나 편차가 심할 때 사용.  
특정 머신러닝 모델의 학습 효율을 증가시키기 위함. (k-Nearest Neighbor, SVM 등 사용 시 필수적)  
해석적 관점에서도 유용. 

표준화, 정규화 외에 이상치에 영향을 덜 받는 RobustScaler 사용 가능.


### 11.5 모델 성능 향상을 위한 파생 변수 생성  
전체 데이터에 대한 파악 및 해당 비즈니스 도메인에 대한 충분한 이해 수반 필요.  
다중공선성 문제에 유의하여 상관분석을 진행하는 것이 중요.

### 11.6 슬라이딩 윈도우 데이터 가공  
예측 모델에서 유용.  
데이터가 충분하지 않을 경우 슬라이딩 윈도우 방법을 통해 많은 데이터셋을 확보하고 학습 데이터의 최근성을 가지도록 유도.

### 11.7 범주형 변수의 가변수 처리  
연속형 변수만 사용가능한 분석기법 사용 시에 고려.

### 11.8 클래스 불균형 문제 해결을 위한 언더샘플링과 오버샘플링

데이터 불균형 문제 해결 방법  

1) 가중치 밸런싱  
: 모델 자체에 중요도가 높은 클래스에 정확도 가중치를 두어, 특정 클래스의 분류 정확도가 높아지도록 조정.

2) 언더샘플링, 오버샘플링  
: 불균형 데이터 자체를 균형이 맞도록 가공한 다음 모델 학습.

-언더샘플링: 큰 비중의 클래스 데이터를 줄이는 방법.  
랜덤 언더샘플링  
EasyEnsemble  
Condensed Nearest Neighbor CNN  

-오버샘플링: 작은 비중의 클래스 데이터를 늘리는 방법.  
랜덤 오버샘플링  
Synthetic Minority Over-Sampling Technique SMOTE  
Adaptive Synthetic Sampling Approach


11.9 데이터 거리 측정 방법  
데이터 거리를 측정하기 전 표준화나 정규화 가공이 필요하다.

유클리드 거리  
![image](https://github.com/user-attachments/assets/fd502b1f-0e34-4525-847a-62417af747d2)

맨해튼 거리  
![image](https://github.com/user-attachments/assets/809eebae-cef6-411d-9020-51510432db89)

민코프스키 거리  
![image](https://github.com/user-attachments/assets/74d21cf5-8c81-4a2c-a23f-5030434cf155)  
(p=1 일 경우 맨해튼거리, p=2 일 경우 유클리드 거리)

체비쇼프 거리  
![image](https://github.com/user-attachments/assets/f1a0affa-8af9-4aad-9f3f-72ee86af4bf5)  
민코프스키 거리의 p값을 무한대로 설정한 것  
군집 간의 최대 거리를 구할 때 사용. 값이 0에 가까울수록 유사한 것.

마할라노비스 거리  
![image](https://github.com/user-attachments/assets/dca3cc4b-9aad-474a-831b-0b2dd0ab21d9)  
유클리드 거리에 공분산을 고려한 거리 측정 방법. 단순 거리에 상관성을 함께 볼 수 있다.  
![image](https://github.com/user-attachments/assets/607abdce-ef19-4d97-a1c1-f0725bc08b77)  
마할라노비스 거리로 측정하면, A-B 간 거리는 A-C간 거리와 같다.

코사인 거리  
(코사인 유사도 공식)  
![image](https://github.com/user-attachments/assets/ca008a43-cdd9-47dd-8518-9bb0b95cd900)  
두 벡터의 사이각을 통해 유사도를 구한 것.  
코사인 유사도는 변수 간 크기가 중요한 경우 적합하지 않고,  
협업 필터링 모델, 문서 간 유사도 측정 시 유용하다.

(1 - 코사인 유사도) 를 통해 코사인 거리로 환산한다.

